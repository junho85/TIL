= 1. 도입 아파치 카프카

== 1.1 이 장의 내용
아파치 카프카로 무엇을 할 수 있는지
탄생 배경과 현재 산업계에 주는 영향

== 1.2 아파치 카프카
여러 대의 분산 서버에서 대량의 데이터를 처리하는 분산 메시징 시스템

메시지(데이터)를 받고, 받은 메시지를 다른 시스템이나 장치에 보내기 위해 사용

여러 시스템과 장치를 연결하는 중요한 역할

* 확장성: 여러 서버로 '확장(scale out) 구성'
* 영속성: 수신한 데이터를 '디스크에 유지'
* 유연성: '연계할 수 있는 제품이 많기' 떄문에 제품이나 시스템을 연결하는 허브 역할
* 신뢰성: '메시지 전달 보증'. 데이터 분실 걱정 노노

원래 높은 처리량. 데이터를 실시간 처리하는 처리 성능에 초점을 둠. 이후 기능과 신뢰성 향상시킴. 종합 스트림 처리를 위한 플랫폼이 되고 있음.

오픈소스. 기업용 엔터프라이즈 버전이 존재하지 않음.

== 1.3 카프카 탄생 배경

=== 1.3.1 링크드인의 시스템 요구 사항
2011년 미국 링크드인(LinkedIn)에서 출발

https://blog.linkedin.com/2011/01/11/open-source-linkedin-kafka[Open-sourcing Kafka, LinkedIn's distributed message queue 2011.01.11]

웹사이트에서 생성되는 로그를 처리하여 웹사이트 활동을 추적하는 것을 목적으로 개발

사용자가 페이지 뷰와 검색 시 키워드 광고의 이용 상황도 포함

웹에서 생성되는 대량의 로그를 분석하여 사용자가 웹에서 하는 활동을 모니터링, 서비스 개선에 활용

빅데이터를 어떻게 활용할 것인지가 큰 화제였던 때. 웹 기업에서는 웹사이트에서 생성되는 로그를 활용하기 시작

링크드인이 실현하려는 목표

1. 높은 처리량으로 실시간 처리
2. 임의의 타이밍에서 데이털르 읽음
3. 다양한 제품과 시스템에 쉽게 연동
4. 메시지를 잃지 않음


==== 1. 높은 처리량으로 실시간 처리
전 세계 사용자의 방대한 엑세스 데이터를 처리하기 위해 우수한 처리량 필요
사용자의 활동을 신속하게 파악. 사용자의 활동에 따라 즉시 피드백하기 위해서 사용자의 활동 단위로 실시간 처리가 가능해야 함.

==== 2. 임의의 타이밍에서 데이털르 읽음
기존 시스템에서 수집한 액세스 로그를 일정 시간마다 배치 처리로 취급하고 싶다는 요구
데이터를 사용하는 타이밍이 반드시 실시간이 아니라 이용 목적에 따라 다를 가능성이 있기 때문에 방대한 데이터를 전달할 때 버퍼 역할도 가능하기를 원함

==== 3. 다양한 제품과 시스템에 쉽게 연동
데이터의 발생원이 되는 데이터 소스와 관련된 시스템이 하나가 아님. 여러 시스템을 통해 데이털르 받아들여야 했음.
이용 목적에 따라 데이터베이스, 데이터 웨어하우스, 아파치 하둡 등 여러 기반이 존재
당시 대량 데이터 처리를 위해 링크드인에서 하둡 사용. 다른 데이터베이스와 데이터 웨어하우스에서 실시하고 있 처리를 모두 하둡에 이식하는 것은 현실적인 방법이 아니었음.

https://ko.wikipedia.org/wiki/%EB%8D%B0%EC%9D%B4%ED%84%B0_%EC%9B%A8%EC%96%B4%ED%95%98%EC%9A%B0%EC%8A%A4[데이터 웨어하우스]
----
데이터 웨어하우스(data warehouse)란 사용자의 의사 결정에 도움을 주기 위하여, 기간시스템의 데이터베이스에 축적된 데이터를 공통의 형식으로 변환해서 관리하는 데이터베이스를 말한다. 줄여서 DW로도 불린다.
----

기존 자산을 활용하기 위해 하둡하고만 연결할 수 있으면 좋다는 것이 아니라, 데이터베이스나 데이터 웨어하우스 등 다른 제품과의 연결을 쉽게 하고 싶다는 요구가 있었다.


==== 4. 메시지를 잃지 않음
취급하는 메시지가 방대하더라도 메시지를 잃어서는 안 됐음. 당초 이용 목적이 사용자 활동을 추적. 한 건 한 건의 활동을 엄격하게 관리하기보다 약간의 중복이 있더라도 잃지 않는 것이 중요.
건마다 엄격하게 관리하면 처리 오버헤드(processing overhead)가 커짐
'높은 처리량으로 실시간 처리'라는 요건과의 균형을 가미하여 현실적으로 제거해도 좋은 것을 찾아야 했음


=== 1.3.2 카프카 이전 제품

포괄적으로 해결할 수 있는 제품은 없었음.

데이털르 전달하거나 데이터를 로드할 때 필요한 제품. 메시지 큐, 로그 수집, ETL 도구

==== 메시지 큐

IBM WebSphere MQ(2014년 IBM MQ로 바뀜)
JSM(Java Messaging System) 사양을 따르는 ActiveMQ
그 외 RabbitMQ

링크드인에서 요구하는 사항과 일치하지 않았음

===== 강력한 전달 보증이 오버 스펙
IBM WebSphere MQ는 메시지 단위로 트랜잭션을 지원
정확히 한 번만 전송되는 것 보증

JMS에도 사양으로 규정되어 있음. commit()/rollback()으로 커밋/롤백할 수 있음

링크드인에서 다루는 로그의 성질을 고려하면 엄격한 트랜잭션 관리는 다소 오버 스펙
'높은 처리량'의 실현이 높은 우선 순위

===== 스케일 아웃이 용이하지 않음
대량의 메시지를 서버 1대로 대응하는 것은 한계. 처음부터 여러 대의 서버를 사용할 것을 전제
메시지 큐 제품에도 클러스터 구성 되는게 있었지만 가용성을 위한 중복 구성에 주안점.
처리 성능을 높이는 목적으로 노들르 추가할 수 있는 스케일 아웃 기능을 전제로 한 제품이 당시에는 없었음

===== 메시지가 대량으로 쌓이는 것을 예상하지 않았음
카프카 이전 메시지 큐에서는 큐에 쌓인 메시지를 즉시 이용되는 것으로 예상. 장시간에 걸쳐 대량으로 축적하는 것은 예상하지 않았음
링크드인에서는 실시간 처리뿐만 아니라 메시지를 배치 처리로 이용하는 것도 가정하고 있었음
일정량의 데이터를 일정 기간마다 묶음으로 받아 데이터 웨어하우스에서 처리하기 위해서는 데이터의 축적 시간을 훨씬 길어야 했지만 기존 메시지 큐로는 감당할 수 없었음.

==== 로그 수집 시스템
페이스북 Scribe, 미국 클라우데라(Cloudera)의 Flume

각 프론트엔드 서버가 로그를 중계용 서버에 전송. 거기서 로그를 수집하여 데이터베이스와 분산 파일씨ㅡ템 HDFS (Hadoop Distributed File System)에 축적

===== HDFS로 데이터 축적과 배치 처리만 고려

===== 알기 쉬운 API가 없다
카프카 이전 제품은 미들웨어 내에서의 구현 사양을 모르면 사용하기 힘들다는 지적이 있었음
https://www.microsoft.com/en-us/research/wp-content/uploads/2017/09/Kafka.pdf



===== 수신하는 쪽이 임의로 메시지를 수신하기 어렵다

push보다 pull

==== ETL 도구
데이터 발생원에서 데이터를 추출하고 필요에 따라 변환해 데이터베이스와 데이터 웨어하우스에 로드하는 기능을 갖추고 있는 ETL
Extract 추출, Transform 변환, Load 로드
DataStage, Interstage, Cosminexus, Informatica PowerCenter, Talend


===== 데이터를 파일 단위로 다룬다

===== 수신하는 쪽이 임의로 메시지를 수신하기 어렵다

== 1.4 카프카로 링크드인 요구 사항 실현하기

.요구 사항
1. 높은 처리량으로 실시간 처리
2. 임의의 타이밍에서 데이털르 읽음
3. 다양한 제품과 시스템에 쉽게 연동
4. 메시지를 잃지 않음

.실현 수단
1. 메시징 모델과 스케일 아웃형 아키텍처
2. 디스크로의 데이터 영속화
3. 이해하기 쉬운 API 제공
4. 전달 보증


=== 1.4.1 메시징 모델과 스케일 아웃

아래 요구사항 만족을 위해
1. 높은 처리량으로 실시간 처리
2. 임의의 타이밍에서 데이털르 읽음
3. 다양한 제품과 시스템에 쉽게 연동

메시징 모델 채용
* Producer: 메시지 생산자
* Broker: 메시지 수집/전달 역할
* Consumer: 메시지 소비자


Producer(생산자) ---(메시지)---> Broker ---(메시지)---> Consumer

==== 큐잉 모델
Broker안에 큐
Producer에서의 메시지가 큐에 담김
Consumer가 큐에서 메시지 추출

하나의 큐에 컨슈머가 여러 개 존재할 수 있음
컨슈머를 여러 개 준비해서 컨슈머에 의한 처리 확장
컨슈머가 메시지를 받으면 다른 컨슈머는 메시지를 받을 수 없음

==== 펍/섭 메시징 모델
Publisher 생산자
Subscriber 컨슈머

퍼블리셔가 서브스크라이버에게 직접 메시지를 보내는 것이 아니라 브로커를 통해 전달
퍼블리셔는 누가 그 메시지를 수신하는지 알 수 없음. Broker에 있는 Topic이라고 불리는 카테고리 안에 메시지를 등록


프로듀서/컨슈머 사이에 브로커를 끼우는 장점
큐잉 모델, 펍/섭 메시징 모델 모두 브로커를 사이에 끼우는 형태의 모델. 변경에 강한 시스템 아키텍처를 만들 수 있는 장점

.프로듀서/컨슈머 모두 접속처를 '하나'로 할 수 있다(수를 줄일 수 있다)
프로듀서는 수신자 상관 없이 브로커에 보내기만 하면 됨
컨슈머도 브로커에서 수신하면 됨
브로커가 존재하지 않으면 프로듀서가 컨슈머에게 메시지 보내려면 다수의 프로듀서와 컨슈머를 모두 연결해야 할 수도 있음
시스템 토폴로지를 모두 이해해야 함. 구성을 변경할 떄는 별도의 제작이 필요. 브로커의 존재는 'NxM'의 시스템 구성을 'N+M'으로 만들어 구성을 단순하게 함

.프로듀서/컨슈머 증감에 대응할 수 있다(네트워크 토폴로지 변경에 강하다).
프로듀서/컨슈머 모두 서로의 존재를 몰라도 되어 증감에 유연하게 대응
프로듀서를 증가시키려면 브로커에만 접속하면 됨
컨슈머도 브로커에 접속함 하면 새로운 수신을 시작
프로듀서/컨슈머 접속 관계에 변경이 생겨도 기존 프로듀서/컨슈머에는 영향받지 않음

* 변경에 강하다
** 접속 시작을 위한 구현 부하가 낮다
** 기존 환경에 영향을 주지 않는다

펍/섭 메시징 모델은 TV나 라디오 전파 수신을 상상하면 이해하기 쉽다.

1개의 토픽에 주목한 경우를 큐잉 모델과 비교하면 여럿이 존재하는 모든 서브스크라이버는 동일한 메시지를 받게 된다.
병렬로 동작하는 복수의 서브스크라이버에 전달할 수 있다는 장점
동일한 메시지에 대한 처리이기 때문에 브로커의 토픽에 축적되는 메시지 그룹 입장에서 보면 처리 능력을 높이는 효과는 없다. 따라서 큐잉 모델과 펍/섭 메시징 모델은 장점과 단점이 공존

=== 1.4.2 카프카 메시징 모델
높은 처리량을 실현하기 위해서는 어떻게 확장성 있는 구성을 할 수 있을지가 관건
따라서 카프카에선느 큐잉 모델에서 실현한 *여러 컨슈머가 분산 처리로 메시지를 소비*하는 모델과 펍/섭 메시징 모델에서 실현한 *여러 서브스크라이버에 동일한 메시지를 전달*하고, *토픽 기반으로 전달 내용을 변경*하는 모델로 되어 있다. 이 모델을 실현하기 위해 '컨슈머 그룹Consumer Group'이라는 개념을 도입하여 컨슈머를 확장 구성할 수 있도록 설계

jnjnjnjnjnjnjnjnjnjn
