:hardbreaks:
= 2023.05.02 2

신정규 - 래블업 주식회사
@inureyes

Understanding Large Language Model at Scale

Lablup Inc. : Make AI Accessible
오픈소스 머신러닝 클러스터 플랫폼: Backend.AI 개발
https://www.backend.ai

언어 모델 밑의 이야기

인공신경망: 3분 요약
* 딥 러닝
- 옆의 층 수가 많은 인공신경망 모델
* 파라미터
- 뉴런간의 연결 관계들
* AI 분산 훈련 / 분산 서비스
- 여러 대의 컴퓨터에 나눠서 훈련 / 서비스하는
* 멀티모달 모델
- 여러 작업에 대해 모델들을 함께 동원하는 메타 모델 또는
- 여러 종류의 데이터를 동시에 취급하는 모델
* 인퍼런스 (추론)
- 훈련이 끝난 딥 러닝 모델에 데이터 주고 답을 받는 것
* 파인 튜닝
- AI 모델을 일단 만들어 두고
- 원하는 목표에 맞도록 훈련을 시켜 최적화하는 과정

파운데이션 모델 - 파인튜닝하지 않은 모델

14. 그 날 보고 들은 것
* 2017년 하반기 구글의 목표: 더이상 언어 모델 성능을 좋게 만드는 것이 아니었음
* 이미 끝난 문제
- 모델은 이미 어떤 영역을 돌파했다.
- (내 삽질은 전부 모델 더 잘 만드는 거였는데...)

* 그럼 무슨 문제를 풀고 있었나?
- 잘 만든 모델이 불러오는 욕구를 어떻게 가라앉힐 것인가?
어떻게 어수룩하게 만들어서 사용자의 만족도를 높일 것인가?

* 그 후 일어난 일들
- 2018년 Duplex 시연 (Google I/O): 대신 전화 걸어서 예약을 잡아주는 봇
- 잘 안 됐음. 왜?
- 위의 이유로... 너무 사람같이 추임새를 넣는 것에 역으로 거부감

== 15. 규모
* '거대' '언어' '모델'이 그래서 뭐냐?

* 얼마나 거대하고
* 언어를 어떻게 처리하고
* 그걸 무슨 수로 서비스로 만드는가?

...

== 18. 언어 모델: 2017~2018년

* 2017년
- 통계적 방법으로 7년간 만들어진 구글 번역 서비스의 성능을
- 4주 동안 인공 신경망을 번역에 도입하는 태스크포스팀의 실험 결과가 능가
- 두 달 후 기존 팀 해체 및 모든 번역 엔진 교체
- 1년 후 모바일에서 오프라인 번역을 인공신경망 기반으로 제공

* 2018년
- 번역기 개발 중, 언어쌍에 상관없이 공통된 인공 신경망 구조가 항상 생긴다는 것을 발견
- 언어 템플릿 신경망+추가적 훈련=번역기를 빠르게 만들 수 있음
- 언중이 만 명 미만인 언어의 번역기도 만들 수 있었음
  - 수백만 문장 쌍 -> 수 천 문장으로 줄어듦
- 이 과정의 부산물
  - Transformer, Universal Sentence Encoder, BERT, Duplex

== 19. 언어 모델: 2019~2020년

* 2019년
- Transformer가 굉장히 일반적인 논리 구조를 만들 수 있음을 발견함
- "언어"가 무엇인가? 에 대한 논의
** 언어는 인간에게는 소통을 위한 도구이지만, 수학적으로는 연관된 정보를 논리에 따라 나열하는 방법
** "언어"를 잘 하게 된다는 것의 의미가 무엇인가?
- XLNet, T5의 등장

* 2020년
- 논리 구조의 집중 포인트 차이
** 정보를 투사하는 것이 중요한가? 정보를 최종적으로 표현하는 것이 중요한가? / BERT vs GPT
- GPT-3의 등장
- 수학적 접근: Transformer는 GNN의 특수 표현형?
** GNN (Graph Neural Network, 2018)은 대상의 관계를 표현하는 그래프를 훈련하는 신경망
** 2021년에 증명

== 20. BERT vs. GPT

* BERT (Bidirectional Encoder Representations from Transformers, Google, 2018)
- 트랜스포머 기반의 인코더 기반
* GPT (Generative Pre-trained Transformer, OpenAI, 2018)
- 트랜스포머 기반의 디코더 기반
- 117M 매개변수
* 차잇점
- Encoder vs. decoder
** Latent Space에 정보를 투사하는 것이 중요한가, 그렇지 않으면 ...

== 21 XLNet (2019)
* XLNet
- 구글의 자동회귀모형 (autogressive model) 기반 언어모델
...

== 22 T5 (Text-To-Text Transfer Transfomer) (2019)
* T5 (Text-To-Text Transfer Transformer, Google, 2019)
- 텍스트-텍스트 변환 모델: 모든 자연어 처리를 텍스트-텍스트 변환 모델로 취급하는 접근
- BERT, GPT와 달리 인코더/디코더 모두에 Transformer 블록 사용
- 모든 자연어 처리를 텍스트-텍스트 변환 모델로 취급하는 접근 방법
* 훈련
- 레이블되지 않은 일반 텍스트 다량 사용
- 이후 레이블된 데이터로 목적성에 맞는 파인튜닝 실행
* 의의
- 일반적으로 훈련을 통해 접근할 수 있는 NLP 모델 중 가장 큰 편에 속함
- (발표 당시는 초거대였음)

== 23. 거대 언어 모델: 2021~2022년
* 모델 키우기
- 왜?
- 크면 해결 되는 일들이 있더라

* 10B (100억 파라미터)
- 거대 언어 모델의 컨텍스트 인식 점프
- RLHF의 이득을 가장 많이 보는 구간

* 100B (1000억 파라미터)
- 거대 언어 모델의 동작을 가르는 지점

== 24. GPT-3 (Generative Pre-trained Transformer 3)
* GPT-3 (2020)
- 당시 가장 큰 모델 크기 (1750억 매개변수)
** 기본적으로는 GPT-2의 확장 버전
** 모델 자체는 공개하지 않았음
...

== 25. 거대 언어 모델: 2021~2022년 / 공개 모델들
* PanGu-a (Huawei, 2021)
- 중국어 단일 언어 모델 중 가장 큰 사이즈 (2000억 파라미터)
- 감정 주제에 대한 폭넓은 대화 지원

* OPT-175B (Meta, 2022)
- 사전 훈련하여 공개한 영문 기반 모델 중 가장 큰 사이즈 (1750억 파라미터)
- 모델 동작 시 Nvidia V100 16장 GPU 요구 (512GB) / 실제 동작시 사용 메모리는 약 350GB (A100 5장)
- 모델 자체보다, 모델을 만들면서 고생한 모든 내용을 기록으로 남겨서 공개한 내용이 심금을 울림

* GLM-130B (칭화대, 2022)
- 중국산 반도체만으로 만들었다고 합니다. (A100 금수 조치 이후 며칠만에 발표)
- 그 이후: A800 들어 보신 분?
** A100에서 NVLink 덜어 낸 기종

== 26. 거대 언어 모델: 2021~2022년 / 서비스들
* Zero-shot 번역 훈련
- 아예 문장 쌍 데이터 없이 번역이 가능할까?
- 24 언어 번역 모델을 zero-shot으로 개발 (Google, 2022)

* Galactica (Meta, 2022)
- 논문 작성 모델 (2022년 11월): 이런 일도 무난하게 할 수 있다!
- 종종 오류를 내는 것으로 비판 받아 사흘만에 공개 종료
- 전략의 실패...

* ChatGPT (OpenAI, 2022)
- InstructGPT 기반의 일반 대화 모델
- 거대 언어 모델 대중화의 문을 열었음

== 27. 거대 언어 모델: 스케일

10MB 스마트폰 내장 언어 번역 모델 용량
100MB 이 발표자료 파일 용량
20GB 일반적인 GPU 메모리 용량 koGPT 인퍼런스 모델 용량
320GB GPT-3 인퍼런스 모델 용량 - A100 GPU 4장
800GB GPT-3.5 / ChatGPT 인퍼런스 모델 용량 (추정) - A100 GPU 10장 TPUv4 Pod 0.6%
8.9TB PaLM 모델 훈련시 요구 용량 (추정) - A100 GPU 112장 Cerebras 1장 TPUv4 Pod 7%

== 33. LaMDA (2021)
* LaMDA : 다이얼로그 어플리케이션을 위한 언어 모델
- 몇백만 가지의 컨텍스트에 대한 상황 인식 및 답변 생성
- 훈련된 개념들에 기초한 답변 생성.
- "발화 주체"를 중심으로 한 개념 정리와 그에 따른 바화 생성

* +멀티모달 모델

* LaMDA 2 (2022)
- 아직 1과의 차잇점이 (공식적으로는) 공개되지 않음
- 지각이 있다고 주장한 연구와 해고 (2022년 7월)

== 34. Pathways (2022)
* Pathways (PaLM, 2022)
- 5400억개의 파라미터 사용
- 수많은 태스크에 범용으로 대응하기 위한 분산 훈련 기반 모델
- Foundation model: 다른...

...


== 40 자원 전쟁의 시대

* 2023년 2월 8일
- 오전 3시 Microsoft: Bing + ChatGPT + Edge + (Windows 11)
** "The race has begun"
- 오후 10시 Google: Bard 발표
** 선빵 필승의 세계

* 승자
- NVIDIA...
- GTC 2023:
- 1년 만에 미국가신 메타버스...

== 41. GPT-3-DaVinci (2022)
* GPT-3.5 DaVinci (OpenAI, 2022)
- GPT-3의 개선 버전
- 인퍼런스에 초점을 두어 크기 감소에도 불구하고 성능 유지에 중점을 둠
- GPT 3의 요약, 번역, Q&A 및 컨텐츠 생성 기능 제공

* InstructGPT 기반
...


== 43. GPT-3.5 / ChatGPT (2022) / new Bing (2023)
...

== 44. Bard (2023)
...

== 45. LLaMA (2023)


Megatron

DeepSpeed

== 61. 거대 언어 모델 훈련하기: "병목"


...


== 74. 2023년의 전망 및 도전 과제

* 2023년 이후의 (래블업 입장에서 보는) 도전 과제들
- 증가하는 딥 러닝 소프트웨어 레거시
** 엔터프라이즈 AI/ML과 신기술 등장의 속도 차
- Arm / RISC-V 등 멀티 아키텍처 플랫폼의 등장
** 인간계의 한계: 발열과 전력 소모 문제
** 전력 대 성능비의 중요성 - 더이상 데이터센터에 집적이 힘들다
- AI 훈련 / 서비스 가속 ASIC의 춘추전국시대 도래
** CPU 대비 상대적으로 간단한 구조
** 특화 기능 대응의 용이함을 살릴 수 있을 것인지?
- ...

== 75. 오늘 이야기
* 거대 언어 모델
- 2017년의 깨달음
- "규모"에 대하여

* 거대 언어 "모델링"
- 언어 모델과 자원
- 적정 모델, 거대 모델
- 대 AI 시대

...

== 76. 마치며
* 2022년 10월 방콕, ML Community Summit (Google)
- JAX, MLIR 등...

* 식사 시간
- Huggingface의 현재 과제: 모델을 필터링할 권한
- 딥 러닝 모델은 자체 편견을 만들지 않을 수 있는가?
- 더 빠르게, 더 공격적으로 만들지 않을까
- 편견 없는 데이터로 편견 간단히 유도하기

- 모델끼리 이런 식으로...
** 근본 없는 xx칩으로 훈련한 주제에!
** 너 몇년산 모델이야? 나 2035년산이야!



AI 정보
아카이브
