:hardbreaks:
= 쏘카 데이터 그룹 글

== 쏘카 데이터 그룹 - Airflow와 함께한 데이터 환경 구축기(feat. Airflow on Kubernetes) 2021.06.01
지난 3년간 Airflow 구축 및 운영기록

https://tech.socarcorp.kr/data/2021/06/01/data-engineering-with-airflow.html

2018년 부터 2021년 까지 어떻게 Airflow를 구축하고 운영했는지

2018 데이터 그룹 설립
기존 쏘카 테크 조직들은 Task 스케줄링 도구로 Rundeck을 이용하고 있었음. 데이터 그룹에서도 이 Rundeck을 그대로 사용할지, 아니면 새로운 도구를 도입해볼지 고민. 다음과 같은 요구사항을 염두해 두었음

1. 데이터 파이프라인(ELT 혹은 ETL) 스케줄링 작업과 쏘카 서비스의 스케줄링 작업 분리를 시켜야 함
* 쏘카 서비스와 관련된 스케쥴링 작업은 실제 고객을 대상으로하는 작업이므로, 데이터 파이프라인보다 우선순위가 높음
* 데이터 파이프라인 작업 때문에 쏘카 서비스에 영향을 주어서는 안 됨
2. 데이터 파이프라인을 위한 다양한 기능이 제공되는 도구 원함
* 쏘카는 데이터 웨어하우스로 BigQuery를 사용하고 있었으므로, BigQuery 및 클라우드와 수비게 연동이 가능한 도구면 좋음
3. Workflow 시각화를 잘해주는 도구 원함
* 많은 데이터 파이프라인이 만들어질텐데, 이 파이프라인들이 어떤 테스크를 수행하는지 한 눈에 확인 가능해야 함

기존 Rundeck은 위와 같은 요구사항을 충족시키기엔 부족했기에, 새로운 도구를 도입하기로 결정했음. 결과적으로 당시 데이터 파이프라인 플랫픔오르 많이 쓰이는 Apache Airflow를 사용해보기로 결정했음

태동기 - Google Cloud Composer

데이터 그룹이 생긴 태동기에는 데이터 엔지니어가 없었음. 데이터 분석가들이 직접 데이터 파이프라인을 만들어야 했음
기술적인 이슈보다 당장 분석에 필요한 파이프라인을 만드는 것이 가장 높은 우선순위였음. 따라서 직접 구축 및 관리가 필요 없는 Google Cloud의 Cloud Composer를 사용하게 되었음

Cloud Composer는 Google Cloud에서 제공하는 Managed Airflow 서비스. 참고로 AWS에도 Managed Airflow(MWAA)가 존재하지만, 그 당시엔 구글 클라우드의 Cloud Composer가 유일했음. 또한 쏘카에선 데이터 웨어하우스를 구글 클라우드의 BigQuery로 사용했기 때문에 같은 구글 클라우드 서비스가 꽤 괜찮은 이점을 줄 거라 생각했음.

Cloud Composer는 정말 간단하게 사용할 수 있음. Airflow의 DAG 파일을 Cloud Storag에 업로드해서 사용함. Cloud Storag에 Airflow DAG 파일을 업로드하면 Cloud Composer에서 해당 파일을 읽어 작업을 실행함. 사용자 입장에서 Airflow 구성 요소에 대해 크게 신경 쓸 필요가 없이 DAG 파일만 잘 만들면 됨.

그러나 Composer는 종종 알 수 없는 에러를 발생시켰음. 당시 Airflow의 버전은 1.10.3로 오픈소스 자체에도 버그가 존재했음. 오류가 생겼을 때 로그를 직접 제대로 볼 수 없는 것이 1.10.3 버전의 가장 큰 문제였음.
Composer의 안정성 문제와 로그와 관련된 이슈로 Composer를 계속 사용해야 하는가에 대한 고민이 시작되었음.

초창기 - Google Computer Engine + Docker Compose

데이터 그룹이 생긴 몇 달 뒤 데이터 엔지니어분들이 데이터 그룹에 합류하게 되었고, 자체 데이터 엔지니어링 팀이 탄생했음. 이제 데이터 엔지니어링팀에서 데이터 파이프라인 구축과 인프라 운영을 담당하게 되었고, 이전보다 좀 더 체계적인 데이터 분석 환경을 구축해볼 수 있게 되었음. 이제 Cloud Composer를 벗어나 데이터 엔지니어링 팀에서 Airflow를 직접 구축하고 운영하기로 하였음.

고려 사항
* 빠르고 간단하게 배포할 수 있어야 함
* 데이터 그룹의 누구나(데이터 분석가/데이터 사이언티스트, 데이터 엔지니어) 쉽게 DAG 작성이 가능해야 함.
* 운영이 수월해야 함

데이터 엔지니어링 팀과 데이터 그룹이 크지 않기 때문에 간단하며 신속하고 유연하게 움직일 수 있는 방식을 고려했음

의사 결정
고민 끝에 선택한 배포 방법은 Google Compute Engine 위에 Docker Compose로 Airflow의 각 컴포넌트 (Webserver, Scheduler 등)를 직접 Docker 컨테이너로 띄우는 방법. Github에 있는 pucket/docker-airflow 이미지를 이용하면 이를 쉽게 구현할 수 있었음.

https://github.com/puckel/docker-airflow

또한 분석 팀원들에게 익숙한 Jupyter Notebook을 동일 환경에서 배포하고, Jupyter Notebok을 통해 Airflow DAG을 수정할 수 있도록 했음

배포 형태
* Google Cloud Platform의 Google Computer Engine 인스턴스를 호스팅 서버로 사용함
* Airflow의 각 컴포넌트인 WebServer, Scheduler, Database, Worker, Redis를 Docker Compose를 통해 각각 컨테이너로 실행함
* Airflow Scheduler는 Celery Executo를 사용함
* Jupyter Notebook을 Docker 컨테이너로 띄운 뒤 Airflwo DAG 폴더에 Volume Mount함

이렇게 하면 Airflow의 각각의 컴포넌트에 대해 통제가 가능해짐 (Docker Container 재시작, 버전 업데이트, 로그 등)

운영 형태
Google Compute Engine에 배포 후, 다음과 같은 방법으로 Airflo를 사용했음

* DAG 작성자는 Jupyter Notebook 웹에 접속해 DAG을 작성함
** Juputer Notebook을 통해 쉽게 코드 작성이 가능함
** Volume Mount 되어 있기 때문에 저장하면 몇 초 내로 Airflo에 반영됨
* Airflwo의 Connections에 BigQuery 설정을 저장해 재사용 가능하게 함
** 이런 설정을 저장하는 일은 주로 데이터 엔지니어링팀에서 담당함
** DAG 작성자는 저장할 설정을 사용함

위와 같은 방식은 Airflo를 간단하게 배포할 수 있었고, 운영 역시 어렵지 않았음. 그러나 조직과 서비스가 점점 성장하면서 여러 문제점이 보이기 시작함

문제점
1) 늘어나는 DAG
데이터와 서비스가 성장하면서 DAG의 수도 점점 늘어났음. DAG의 수가 늘어나다 보니 DAG의 실행 속도도 점점 느려지고, 제시간에 실행되지 않고 시작 시간이 밀리는 경우도 발생했음. 특히 배치 주기가 짧은 DAG에서 이런 현상은 매우 심각한 문제였음.

이런 경우에 할 수 있는 일은 Google Computer Engine의 머신 유형을 한 단계 업그레이드(컴퓨팅 리소스를 수직 확장, Scale Up)하는 방법뿐이었음. 그러나 이 방법을 선택해도 DAG이 실행되는 특정 시간 외에는 리소스를 쓰는 일이 거의 없기 때문에 리소스 사용 효율 측면에서는 매우 비효율적. 리소스를 유연하고 효율적으로 사용할 수 있는 방법에 대해 고민이 들기 시작

2) 규칙 없이 제각각으로 작성된 DAG 코드
* 여러 사람이 모두 제각각의 스타일로 작성한 DAG을 작성하였음. 예를 들어 어떤 DAG에는 on_failure_callback 값이 있는 반면, 어떤 DAG들은 이 값들이 보이지 않았음.
* Jupyter Notebook 환경에서 코드를 작성하다 보니 PEP8과 같은 스타일 컨벤션을 잘 지키긴 어려웠음
* 가장 큰 문제는 DAG 코드가 담긴 모듈 이름에 규칙이 없어서, Airflow 웹에서 문제가 생긴 DAG이 실제로 어떤 파일인지 찾기 쉽지 않았음.

일관된 DAG의 템플릿과 PEP8과 같은 코드 스타일 규칙의 필요성이 느껴졌음. 또한 코드 변경을 추적할 수 있도록 버전 관리 도구(Git 등)의 필요성을 느끼기 시작했음.
