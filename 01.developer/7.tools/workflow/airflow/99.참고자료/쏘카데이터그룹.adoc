:hardbreaks:
= 쏘카 데이터 그룹 글

== 쏘카 데이터 그룹 - Airflow와 함께한 데이터 환경 구축기(feat. Airflow on Kubernetes) 2021.06.01
지난 3년간 Airflow 구축 및 운영기록

https://tech.socarcorp.kr/data/2021/06/01/data-engineering-with-airflow.html

2018년 부터 2021년 까지 어떻게 Airflow를 구축하고 운영했는지

2018 데이터 그룹 설립
기존 쏘카 테크 조직들은 Task 스케줄링 도구로 Rundeck을 이용하고 있었음. 데이터 그룹에서도 이 Rundeck을 그대로 사용할지, 아니면 새로운 도구를 도입해볼지 고민. 다음과 같은 요구사항을 염두해 두었음

1. 데이터 파이프라인(ELT 혹은 ETL) 스케줄링 작업과 쏘카 서비스의 스케줄링 작업 분리를 시켜야 함
* 쏘카 서비스와 관련된 스케쥴링 작업은 실제 고객을 대상으로하는 작업이므로, 데이터 파이프라인보다 우선순위가 높음
* 데이터 파이프라인 작업 때문에 쏘카 서비스에 영향을 주어서는 안 됨
2. 데이터 파이프라인을 위한 다양한 기능이 제공되는 도구 원함
* 쏘카는 데이터 웨어하우스로 BigQuery를 사용하고 있었으므로, BigQuery 및 클라우드와 수비게 연동이 가능한 도구면 좋음
3. Workflow 시각화를 잘해주는 도구 원함
* 많은 데이터 파이프라인이 만들어질텐데, 이 파이프라인들이 어떤 테스크를 수행하는지 한 눈에 확인 가능해야 함

기존 Rundeck은 위와 같은 요구사항을 충족시키기엔 부족했기에, 새로운 도구를 도입하기로 결정했음. 결과적으로 당시 데이터 파이프라인 플랫픔오르 많이 쓰이는 Apache Airflow를 사용해보기로 결정했음

태동기 - Google Cloud Composer

데이터 그룹이 생긴 태동기에는 데이터 엔지니어가 없었음. 데이터 분석가들이 직접 데이터 파이프라인을 만들어야 했음
기술적인 이슈보다 당장 분석에 필요한 파이프라인을 만드는 것이 가장 높은 우선순위였음. 따라서 직접 구축 및 관리가 필요 없는 Google Cloud의 Cloud Composer를 사용하게 되었음

Cloud Composer는 Google Cloud에서 제공하는 Managed Airflow 서비스. 참고로 AWS에도 Managed Airflow(MWAA)가 존재하지만, 그 당시엔 구글 클라우드의 Cloud Composer가 유일했음. 또한 쏘카에선 데이터 웨어하우스를 구글 클라우드의 BigQuery로 사용했기 때문에 같은 구글 클라우드 서비스가 꽤 괜찮은 이점을 줄 거라 생각했음.

Cloud Composer는 정말 간단하게 사용할 수 있음. Airflow의 DAG 파일을 Cloud Storag에 업로드해서 사용함. Cloud Storag에 Airflow DAG 파일을 업로드하면 Cloud Composer에서 해당 파일을 읽어 작업을 실행함. 사용자 입장에서 Airflow 구성 요소에 대해 크게 신경 쓸 필요가 없이 DAG 파일만 잘 만들면 됨.

그러나 Composer는 종종 알 수 없는 에러를 발생시켰음. 당시 Airflow의 버전은 1.10.3로 오픈소스 자체에도 버그가 존재했음. 오류가 생겼을 때 로그를 직접 제대로 볼 수 없는 것이 1.10.3 버전의 가장 큰 문제였음.
Composer의 안정성 문제와 로그와 관련된 이슈로 Composer를 계속 사용해야 하는가에 대한 고민이 시작되었음.

초창기 - Google Computer Engine + Docker Compose

데이터 그룹이 생긴 몇 달 뒤 데이터 엔지니어분들이 데이터 그룹에 합류하게 되었고, 자체 데이터 엔지니어링 팀이 탄생했음. 이제 데이터 엔지니어링팀에서 데이터 파이프라인 구축과 인프라 운영을 담당하게 되었고, 이전보다 좀 더 체계적인 데이터 분석 환경을 구축해볼 수 있게 되었음. 이제 Cloud Composer를 벗어나 데이터 엔지니어링 팀에서 Airflow를 직접 구축하고 운영하기로 하였음.

고려 사항
* 빠르고 간단하게 배포할 수 있어야 함
* 데이터 그룹의 누구나(데이터 분석가/데이터 사이언티스트, 데이터 엔지니어) 쉽게 DAG 작성이 가능해야 함.
* 운영이 수월해야 함

데이터 엔지니어링 팀과 데이터 그룹이 크지 않기 때문에 간단하며 신속하고 유연하게 움직일 수 있는 방식을 고려했음

의사 결정
고민 끝에 선택한 배포 방법은 Google Compute Engine 위에 Docker Compose로 Airflow의 각 컴포넌트 (Webserver, Scheduler 등)를 직접 Docker 컨테이너로 띄우는 방법. Github에 있는 pucket/docker-airflow 이미지를 이용하면 이를 쉽게 구현할 수 있었음.

https://github.com/puckel/docker-airflow

또한 분석 팀원들에게 익숙한 Jupyter Notebook을 동일 환경에서 배포하고, Jupyter Notebok을 통해 Airflow DAG을 수정할 수 있도록 했음

배포 형태
* Google Cloud Platform의 Google Computer Engine 인스턴스를 호스팅 서버로 사용함
* Airflow의 각 컴포넌트인 WebServer, Scheduler, Database, Worker, Redis를 Docker Compose를 통해 각각 컨테이너로 실행함
* Airflow Scheduler는 Celery Executo를 사용함
* Jupyter Notebook을 Docker 컨테이너로 띄운 뒤 Airflwo DAG 폴더에 Volume Mount함

이렇게 하면 Airflow의 각각의 컴포넌트에 대해 통제가 가능해짐 (Docker Container 재시작, 버전 업데이트, 로그 등)

운영 형태
Google Compute Engine에 배포 후, 다음과 같은 방법으로 Airflo를 사용했음

* DAG 작성자는 Jupyter Notebook 웹에 접속해 DAG을 작성함
** Juputer Notebook을 통해 쉽게 코드 작성이 가능함
** Volume Mount 되어 있기 때문에 저장하면 몇 초 내로 Airflo에 반영됨
* Airflwo의 Connections에 BigQuery 설정을 저장해 재사용 가능하게 함
** 이런 설정을 저장하는 일은 주로 데이터 엔지니어링팀에서 담당함
** DAG 작성자는 저장할 설정을 사용함

위와 같은 방식은 Airflo를 간단하게 배포할 수 있었고, 운영 역시 어렵지 않았음. 그러나 조직과 서비스가 점점 성장하면서 여러 문제점이 보이기 시작함

문제점
1) 늘어나는 DAG
데이터와 서비스가 성장하면서 DAG의 수도 점점 늘어났음. DAG의 수가 늘어나다 보니 DAG의 실행 속도도 점점 느려지고, 제시간에 실행되지 않고 시작 시간이 밀리는 경우도 발생했음. 특히 배치 주기가 짧은 DAG에서 이런 현상은 매우 심각한 문제였음.

이런 경우에 할 수 있는 일은 Google Computer Engine의 머신 유형을 한 단계 업그레이드(컴퓨팅 리소스를 수직 확장, Scale Up)하는 방법뿐이었음. 그러나 이 방법을 선택해도 DAG이 실행되는 특정 시간 외에는 리소스를 쓰는 일이 거의 없기 때문에 리소스 사용 효율 측면에서는 매우 비효율적. 리소스를 유연하고 효율적으로 사용할 수 있는 방법에 대해 고민이 들기 시작

2) 규칙 없이 제각각으로 작성된 DAG 코드
* 여러 사람이 모두 제각각의 스타일로 작성한 DAG을 작성하였음. 예를 들어 어떤 DAG에는 on_failure_callback 값이 있는 반면, 어떤 DAG들은 이 값들이 보이지 않았음.
* Jupyter Notebook 환경에서 코드를 작성하다 보니 PEP8과 같은 스타일 컨벤션을 잘 지키긴 어려웠음
* 가장 큰 문제는 DAG 코드가 담긴 모듈 이름에 규칙이 없어서, Airflow 웹에서 문제가 생긴 DAG이 실제로 어떤 파일인지 찾기 쉽지 않았음.

일관된 DAG의 템플릿과 PEP8과 같은 코드 스타일 규칙의 필요성이 느껴졌음. 또한 코드 변경을 추적할 수 있도록 버전 관리 도구(Git 등)의 필요성을 느끼기 시작했음.


3) 복잡해지는 의존성 문제
Airflow의 버전이 바뀌면 기존 DAG코드도 변경해야 하는 문제가 있음. Airflow 버전은 생각보다 빠르게 업데이트되었고, 그때마다 의존성/안정성의 문제로 쉽게 Airflow의 버전을 올리기 어려웠음

그리고 일부 DAG에서 requests 나 beautifulsoup4, lxml 와 같은 라이브러리를 사용하는 경우가 있음. 이런 경우 Airflow Webserver, Scheduler, Worker 컨테이너에 모두 라이브러리를 설치해야 했음. 설치된 Airflow 버전과 호환이 되어야 했기 때문에 파이썬 버전도 신경 써야 했음. 이렇게 가다간 흔히 말하는 의존성 지옥에 빠질 거 같았음.

Airflow, 라이브러리, 파이썬 버전 등 점점 복잡해지는 의존성을 더 쉽게 해결할 방법을 찾아야 했음.

4) 운영과 테스트의 혼재
이 당시 Airflow상에 있는 DAG은 대부분 Data Lake나 Data Mar를 만드는 파이프라인이 많았고, 이 파이프라인들의 목적지는 주로 BigQuery였음. 대부분의 DAG 작성자들은 파이프라인이 잘 작동하는지 테스트하기 위해 BigQuery의 최종 테이블 이름 끝에 _test를 붙여서 실행하는 경우가 많았음. 이렇게 실행 후, 테스트한 데이터셋을 지우지 않는 경우가 많아 BigQuery에 _test로 끝나는 테이블들이 너무 많이 생기게 되었음.

그리고 테스트하는 과정에서 실제 운영 중인 테이블에 직접 접근하는 로직이 포함도니 DAG이 있는데, 이 DAG들이 가장 큰 문제였음. 예를 들어 DAG 작성자가 BigqueryOperator를 사용해 테스트할 때, 깜빡하고 테이블 이름 끝에 _test 를 붙이지 않고 DAG을 실행하는 경우, 실제 운영되는 테이블을 덮어쓰거나 과거 데이터를 날려버릴 수 있음.

운영 환경과 격리되어 안전하게 테스트할 수 있도록 별도의 개발 환경이 필요했음.

성장기 - Airflow on Kubernetes

데이터 그룹이 생긴 이후 2년 동안 회사는 빠르게 성장했고, 데이터 그룹의 인원도 7명에서 27명으로 늘어나게 되었음. Airflow에서도 다양한 DAG이 생기게 되었고, 그 개수는 600개를 넘어섰음. 또한 데이터 그룹의 데이터 분석가 모두 Airflow DAG을 작성할 수 있었기 때문에 DAG을 작성하는 사람들도 많아지고, 사용하는 Operator도 더 다양해졌음. 더 많은 리소스가 필요하게 되었고, 관리할 포인트가 점점 더 생겼음.

고려 사항
위에서 생긴 문제들을 해결할 방안을 생각해야 했습니다. 구체적으로는 다음 관점으로 Airflow 고도화에 대해 생각했음

* DAG이 늘어나도 실행 시간이 늘어나거나 지연이 없도록 리소스를 유연하게 확보 및 할당할 수 있어야 함
* 깔끔하고 일관된 DAG 코드와 파라미터값을 표준화해야 함
* Airflow와 파이썬, 파이썬 라이브러리의 의존성을 최대한 낮춰야 함
* 운영 환경에 영향을 주지 않고 테스트 가능한 별도이 환경이 필요함

해결 방법
다양한 고려 사항을 기반으로 다음과 같은 해결 방법을 세우고, 하나씩 도입하기 시작

* 단일 컴퓨팅이 아닌 Kubernetes 도입
** Node Auto Scaling을 적용해 필요에 따라 리소스를 유연하게 확보하고 할당할 수 있음
** 사용할 노드 풀을 직접 관리하기 때문에 CPU, GPU 사용량이 높은 파이프라인을 실행하기 좋음
** Airflow 외에도 데이터 그룹에 필요한 다른 서비스들도 이 클러스터에서 관리할 수 있음.
* Airflow를 1.10.14 버전으로 업데이트하고, Kubernetes Executor를 사용함
** Task 단위로 Pod를 생성하기 때문에 동시에 여러 Task를 빠르게 실행할 수 있음
** Node Auto Scaling이 적용되어 있어 DAG이 늘어나도 리소스로 인한 실행 속도에 문제가 없음
* 특정 환경(라이브러리, 파이썬 버전 등)의 의존성이 강한 DAG은 KubernetesPodOperator를 사용함
** KubernetesPodOperator는 Airflow 1.10.x 버전부터 등장한 어퍼레이터로, 파라미터로 컨테이너 이미지를 입력받고 해당 이미지를 실행함
** 즉, 실행할 프로그램을 Airflow에 의존성 없이 개발할 수 있고, 이를 컨테이너 이미지로 잘 말아두면 KubernetesPodOperator로 실행할 수 있음
* DAG 코드 작성에 규칙을 생성함
** DAG 템플릿을 추상화해서 몇 개만 입력하면 DAG 코드를 생성할 수 있는 CLI를 생성했음. 이 CLI를 사용해 규격화된 DAG 파일을 생성함
** Code Formatter로 Black을 사용하고, CI 과정에서 포매팅을 검사함
* DAG 코드를 Git과 Github로 버전 관리 함
** 누가 언제 어떤 코드를 추가, 변경했는지 관리할 수 있음.
** 코드 리뷰를 통해 보다 나은 DAG 코드를 유지함
* 개발용 Google Cloud Platform 프로젝트에 격리된 테스트용 Airflow 환경ㅇ르 만듬
** DAG 코드가 담긴 Github Repository의 브랜치에 따라 동적으로 테스트용 Airflow를 배포함
*** 예를 들면 feature/hardy 라는 브랜치를 만들면 hardy 용 Airflow가 배포됨
*** 이런 방식으로 자신의 DAG을 테스트할 Airflow를 할당받을 수 있음
*** 격리된 환경이기 때문에 다른 사람의 작업 내용에 영향받지 않고 작업할 수 있음
** 개발 환경에서는 운영 BigQuery에 READ만 가능하도록 함
*** 자세한 내용은 아래에서 더 설명

Kubernetes 환경을 도입하면 여러 장점이 많아지지만, 운영의 복잡도와 난이도가 높아짐. 2년 전과 비교해 Cloud Composer도 많이 성숙해졌고 AWS MWAA도 등장해 이 서비스를 다시 활용하는 것도 고려해보았으나, 데이터 그룹에서 사용할 Kubernetes 클러스터를 직접 만들고 이 위에서 여러 서비스를 운영하기로 결정했음

데이터 엔지니어링 팀도 점점 커지고 있어 관리할 수 있는 사람이 많아졌고, 점점 더 복잡해질 인프라 구축에 맞서서 노력할 준비가 되어있었음

배포 형태(운영 환경)
운영과 개발 환경의 배포 형태가 미세하지만 다르게 만들었음.

먼저 운영(Prod) 환경은 개발(Dev) 환경과 다른 별도의 Google Cloud Platform 프로젝트에서 구성됨
형태는 다음과 같음


... TODO




